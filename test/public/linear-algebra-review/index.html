
<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Linear Algebra Review | Nerd-Emoji Web</title>
    <style>
      body {
          font-family: 'Open Sans'
      }
      </style>
    <link rel="stylesheet" href="https://thecarsonwest.github.io/css/style.css" />
    <link rel="icon" type="image/x-icon" href="https://thecarsonwest.github.io/nerd-emoji.ico">
    <link href='https://fonts.googleapis.com/css?family=Open Sans' rel='stylesheet'>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS_CHTML"></script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$']]
    },
    svg: {
      fontCache: 'global'
    }
  };
  </script>
    
  </head>

  <body>

    <nav>
    <ul class="menu">
      
    </ul>
    <hr/>
    </nav>
<div class="article-meta">
<h1><span class="title">Linear Algebra Review</span></h1>
<h2 class="author">Carson West</h2>

</div>

<main>
<h1 id="exam-prep-schedule">[[Exam Prep Schedule]]</h1>
<h1 id="linear-algebra-review">[[Linear Algebra Review]]</h1>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Vectors:</strong>  Ordered lists of numbers.  Represented as column vectors:   $  \begin{bmatrix} x_1 \ x_2 \ \vdots \ x_n \end{bmatrix}  $   or row vectors:  $  \begin{bmatrix} x_1 &amp; x_2 &amp; \cdots &amp; x_n \end{bmatrix}  $ .  [[Vector Operations]]</li>
<li><strong>Matrices:</strong> Rectangular arrays of numbers.   $ A = \begin{bmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \end{bmatrix} $  [[Matrix Operations]]</li>
<li><strong>Linear Transformations:</strong> Functions that map vectors to vectors in a linear way.   $ T(\mathbf{x} + \mathbf{y}) = T(\mathbf{x}) + T(\mathbf{y}) $  and  $ T(c\mathbf{x}) = cT(\mathbf{x}) $  for scalars  $ c $ .  Represented by matrices. [[Linear Transformations and Matrices]]</li>
<li><strong>Vector Spaces:</strong>  A collection of vectors that is closed under vector addition and scalar multiplication. [[Vector Spaces]]</li>
<li><strong>Linear Independence:</strong> A set of vectors is linearly independent if no vector can be written as a linear combination of the others.   $ c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_n\mathbf{v}_n = \mathbf{0} $  implies  $ c_1 = c_2 = \cdots = c_n = 0 $ . [[Linear Independence and Dependence]]</li>
<li><strong>Span:</strong> The set of all possible linear combinations of a set of vectors. [[Span of Vectors]]</li>
<li><strong>Basis:</strong> A linearly independent set of vectors that spans the entire vector space. [[Basis and Dimension]]</li>
<li><strong>Dimension:</strong> The number of vectors in a basis.</li>
<li><strong>Eigenvalues and Eigenvectors:</strong>  For a square matrix  $ A $ , an eigenvector  $ \mathbf{v} $  satisfies  $ A\mathbf{v} = \lambda\mathbf{v} $ , where  $ \lambda $  is the corresponding eigenvalue.  [[Eigenvalues and Eigenvectors]]</li>
<li><strong>Determinant:</strong> A scalar value associated with a square matrix.   $ det(A) = 0 $  if and only if the matrix is singular (non-invertible).  [[Determinants]]</li>
<li><strong>Rank:</strong> The dimension of the column space (or row space) of a matrix.  [[Rank of a Matrix]]</li>
<li><strong>Null Space (Kernel):</strong> The set of all vectors  $ \mathbf{x} $  such that  $ A\mathbf{x} = \mathbf{0} $ . [[Null Space]]</li>
<li><strong>Column Space (Range):</strong> The span of the column vectors of a matrix. [[Column Space]]</li>
<li><strong>Row Space:</strong> The span of the row vectors of a matrix. [[Row Space]]</li>
<li><strong>Orthogonality:</strong> Two vectors  $ \mathbf{u} $  and  $ \mathbf{v} $  are orthogonal if their dot product is zero:  $ \mathbf{u} \cdot \mathbf{v} = 0 $ . [[Orthogonality and Orthonormal Bases]]</li>
<li><strong>Gram-Schmidt Process:</strong> A method for orthonormalizing a set of linearly independent vectors. [[Gram-Schmidt Process]]</li>
<li><strong>Inner Product:</strong> A generalization of the dot product to more abstract vector spaces. [[Inner Product Spaces]]</li>
<li><strong>Singular Value Decomposition (SVD):</strong> A factorization of a matrix into three matrices with special properties. [[Singular Value Decomposition]]</li>
</ul>
<p><strong>Important Theorems:</strong></p>
<ul>
<li><strong>Rank-Nullity Theorem:</strong>  For a matrix A,  $ rank(A) + nullity(A) = number\ of\ columns\ in\ A $ .</li>
</ul>
<p><strong>Applications:</strong></p>
<ul>
<li>[[Linear Algebra Applications in Machine Learning]]</li>
<li>[[Linear Algebra Applications in Computer Graphics]]</li>
<li>[[Linear Algebra Applications in Physics]]</li>
</ul>
<p><strong>References:</strong></p>
<ul>
<li>Linear Algebra and its Applications by David C. Lay</li>
</ul>

</main>

  <footer>
  
  
  </footer>
  </body>
</html>

