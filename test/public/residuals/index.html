
<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Residuals | Nerd-Emoji Web</title>
    <style>
      body {
          font-family: 'Open Sans'
      }
      </style>
    <link rel="stylesheet" href="https://thecarsonwest.github.io/css/style.css" />
    <link rel="icon" type="image/x-icon" href="https://thecarsonwest.github.io/nerd-emoji.ico">
    <link href='https://fonts.googleapis.com/css?family=Open Sans' rel='stylesheet'>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  </head>

  <body>

    <nav>
    <ul class="menu">
      
    </ul>
    <hr/>
    </nav>
<div class="article-meta">
<h1><span class="title">Residuals</span></h1>
<h2 class="author">Carson West</h2>

</div>

<main>
<h1 id="ap-stats-home">[[AP Stats Home]]</h1>
<h1 id="residuals">Residuals</h1>
<p>Residuals are fundamental to understanding the accuracy and appropriateness of a [[Linear Regression Models]] when fitting data. They represent the &ldquo;leftover&rdquo; variation in the dependent variable after accounting for the linear relationship with the independent variable.</p>
<h2 id="definition-and-purpose">Definition and Purpose</h2>
<p>A <strong>residual</strong> is the difference between an observed value of the response variable ( $ y $ ) and the value predicted by the regression model ( $ \hat{y} $ ). In essence, it tells us how far off our prediction was for a specific data point.</p>
<ul>
<li><strong>Observed Value ( $ y $ )</strong>: The actual value from the dataset.</li>
<li><strong>Predicted Value ( $ \hat{y} $ )</strong>: The value estimated by the [[Least Squares Regression]] line for a given x-value.</li>
</ul>
<p>The primary purpose of residuals is to help us assess how well a regression model fits the data and to identify any patterns or issues that the linear model might not be capturing.</p>
<h2 id="calculating-residuals">Calculating Residuals</h2>
<p>The formula for a residual ( $ e $ ) is straightforward:</p>
<p>$$  e = y - \hat{y}
$$<br>
Where:</p>
<ul>
<li>$ e $  is the residual</li>
<li>$ y $  is the observed value of the response variable</li>
<li>$ \hat{y} $  is the predicted value of the response variable from the regression line.</li>
</ul>
<p><strong>Example:</strong>
If the actual weight of a dog is 25 kg, and our regression model predicts 22 kg for a dog of that size, the residual would be  $ e = 25 - 22 = 3 $  kg. A positive residual means the model underpredicted the actual value, while a negative residual means the model overpredicted it.</p>
<h2 id="residual-plots">Residual Plots</h2>
<p>A [[Residuals]] plot (or residual plot) is a scatterplot of the residuals against the explanatory variable (x) or the predicted values ( $ \hat{y} $ ). These plots are crucial for assessing the appropriateness of a linear model.</p>
<h3 id="how-to-construct-a-residual-plot">How to Construct a Residual Plot</h3>
<ol>
<li>Calculate the residuals for each data point using the formula  $ e = y - \hat{y} $ .</li>
<li>Plot the residuals ( $ e $ ) on the y-axis against the corresponding explanatory variable ( $ x $ ) or predicted value ( $ \hat{y} $ ) on the x-axis. A horizontal line at  $ y=0 $  is usually added to the plot.</li>
</ol>
<h3 id="interpreting-residual-plots">Interpreting Residual Plots</h3>
<p>When examining a residual plot, we look for two main characteristics:</p>
<ol>
<li>
<p><strong>No Obvious Pattern</strong>:</p>
<ul>
<li><strong>Good Fit (Random Scatter)</strong>: If the points in the residual plot are randomly scattered around the horizontal line at 0 with no discernible pattern, it suggests that a linear model is appropriate for the data. This indicates that the linear model has captured most of the systematic relationship between the variables.</li>
<li><strong>Bad Fit (Presence of a Pattern)</strong>: If there is a clear pattern (e.g., a curved shape, fanning out, or clustering) in the residual plot, it indicates that the linear model is not the best fit for the data. This means a nonlinear relationship might exist, or other variables might be influencing the relationship. [[Analyzing Departures from Linearity]] often involves examining residual plots for patterns.</li>
</ul>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Pattern Type</th>
          <th style="text-align: left">Implication for Linear Model</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Random Scatter</td>
          <td style="text-align: left">Linear model is appropriate.</td>
      </tr>
      <tr>
          <td style="text-align: left">Curved Pattern</td>
          <td style="text-align: left">A non-linear model might be more appropriate.</td>
      </tr>
      <tr>
          <td style="text-align: left">&ldquo;Cone&rdquo; Shape</td>
          <td style="text-align: left">Variability in residuals changes with x; assumptions violated.</td>
      </tr>
      <tr>
          <td style="text-align: left">Horizontal Bands</td>
          <td style="text-align: left">May indicate categorical explanatory variable or other issues.</td>
      </tr>
  </tbody>
</table>
</li>
<li>
<p><strong>Constant Variability (Homoscedasticity)</strong>:</p>
<ul>
<li><strong>Consistent Spread</strong>: The spread of the residuals should be roughly the same across all values of  $ x $  (or  $ \hat{y} $ ). This is known as <strong>homoscedasticity</strong>.</li>
<li><strong>Varying Spread (Heteroscedasticity)</strong>: If the spread of the residuals increases or decreases as  $ x $  (or  $ \hat{y} $ ) increases (often appearing as a &ldquo;cone&rdquo; or &ldquo;fan&rdquo; shape), this indicates <strong>heteroscedasticity</strong>. This violates an important assumption of linear regression and suggests that the precision of our predictions varies across the range of the explanatory variable.</li>
</ul>
</li>
</ol>
<h2 id="properties-of-residuals-in-least-squares-regression">Properties of Residuals in Least Squares Regression</h2>
<p>For a [[Least Squares Regression]] line:</p>
<ul>
<li>The sum of the residuals is always zero:  $ \sum e_i = \sum (y_i - \hat{y}_i) = 0 $ .</li>
<li>The mean of the residuals is also zero.</li>
</ul>
<p>These properties are a direct consequence of how the least squares line is determined, minimizing the sum of squared residuals.</p>

</main>


  </body>
</html>

